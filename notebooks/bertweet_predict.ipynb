{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweetInteraction.ipynb\n",
    "\n",
    "### Designation: Demonstration/Debugging/Validation Script\n",
    "\n",
    "    Purpose: load in the model, weights, and datasets specified, and generate prediction results as well as any additional staticstics.\n",
    "\n",
    "- Requirements:\n",
    "    \n",
    "    Packages: tensorflow, pandas, matplotlib, transformers, sklearn, os\n",
    "\n",
    "    Datasets (csv's) (selectable): trainingData.csv, testingData.csv (default).\n",
    "\n",
    "    Model: bertweet9010.h5 (default, we can use others)\n",
    "\n",
    "- This program will require an internet connection, as it will download the model and tokenizer from the HuggingFace model repository.\n",
    "\n",
    "    - Please note, all files referenced (input and output) will all be on the folder-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on generating predictions\n",
    "- Although it would not be required to use a GPU for this, but I would still suggest using one for this, if it is available.\n",
    "- Please refer to BERTweet.ipynb for explanation on the common steps, detailed explanation will be given for parts that are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Standalone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useCPU = False #Choose whether to use CPU or GPU for running the program\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "if useCPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing, downloading, and Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 134309376 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592899    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,902,275\n",
      "Trainable params: 134,902,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "with tf.device('/GPU:0'):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\",num_labels=3,problem_type=\"multi_label_classification\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",num_labels=3)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bertweet9010.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CSV\n",
    "\n",
    "- selectable: testing data, or training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26493</td>\n",
       "      <td>I started X-Slimmer at eight this morning, it`...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11488</td>\n",
       "      <td>why won`t the kids sleep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7782</td>\n",
       "      <td>Jennnnnn richhhh wast to the ed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18241</td>\n",
       "      <td>LOL! I hate when that happens!! All hyped up ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25540</td>\n",
       "      <td>Stuck on NJ Transit for the past twenty minute...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>11645</td>\n",
       "      <td>Yes. Nag twitter. HAHA  Thanks. LM.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>23533</td>\n",
       "      <td>hm, this not a good medium for much more then...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>6578</td>\n",
       "      <td>hapee mother`s day t all the mothers out there!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>5338</td>\n",
       "      <td>Aww maybe i traumatized her.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>21793</td>\n",
       "      <td>_caruso So I took the polish off of the nail o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2749 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                        DATA_COLUMN  \\\n",
       "0          26493  I started X-Slimmer at eight this morning, it`...   \n",
       "1          11488                           why won`t the kids sleep   \n",
       "2           7782                    Jennnnnn richhhh wast to the ed   \n",
       "3          18241   LOL! I hate when that happens!! All hyped up ...   \n",
       "4          25540  Stuck on NJ Transit for the past twenty minute...   \n",
       "...          ...                                                ...   \n",
       "2744       11645                Yes. Nag twitter. HAHA  Thanks. LM.   \n",
       "2745       23533   hm, this not a good medium for much more then...   \n",
       "2746        6578    hapee mother`s day t all the mothers out there!   \n",
       "2747        5338                       Aww maybe i traumatized her.   \n",
       "2748       21793  _caruso So I took the polish off of the nail o...   \n",
       "\n",
       "      LABEL_COLUMN  \n",
       "0                0  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "2744             2  \n",
       "2745             0  \n",
       "2746             2  \n",
       "2747             0  \n",
       "2748             0  \n",
       "\n",
       "[2749 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRaw = pd.read_csv('testingData.csv')\n",
    "#dfRaw = pd.read_csv('trainingData.csv')\n",
    "dfRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Extract only the text into a new Dataframe, and fill the label column with 0s.\n",
    "\n",
    "- The program will not use the label column, as it is predicting only based on the text. Having it allows us to use already-written methods to put it into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['DATA_COLUMN'] = dfRaw.iloc[:,1].astype(str)\n",
    "df['LABEL_COLUMN'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing, Converting dataframes into supported input formats.\n",
    "\n",
    "- note: there is only 1 dataframe that needs to be converted.\n",
    "\n",
    "- Also note: we are using batch size 1 here (at the end), so we are evaluating each and every query individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def convertToExamples(data,DATA_COLUMN,LABEL_COLUMN):\n",
    "    dataOut = data.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                        text_a = x[DATA_COLUMN], \n",
    "                        text_b = None,\n",
    "                        label = x[LABEL_COLUMN]), axis = 1)\n",
    "    return dataOut \n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "dataInputExamples = convertToExamples(df,'DATA_COLUMN','LABEL_COLUMN')\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    dataTensor = convert_examples_to_tf_dataset(list(dataInputExamples), tokenizer)\n",
    "    dataTensor = dataTensor.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict\n",
    "\n",
    "- instead of training here, we call model.predict() to get the outputs of our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 2.714901  , -0.3710036 , -2.3270257 ],\n",
       "       [ 0.6390394 ,  1.6838382 , -2.4399874 ],\n",
       "       [-0.578817  ,  2.3400283 , -1.958418  ],\n",
       "       ...,\n",
       "       [-2.406961  , -0.56785583,  2.9865649 ],\n",
       "       [ 2.5711346 , -0.45077664, -2.14151   ],\n",
       "       [ 2.9598632 , -0.7908993 , -2.1495836 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "    out = model.predict(dataTensor)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert our logits numpy array into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDF = pd.DataFrame(out['logits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for every row, if a column has the highest number, record that column number, as that is what our AI thinks the category of that query is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2744    2\n",
       "2745    0\n",
       "2746    2\n",
       "2747    0\n",
       "2748    0\n",
       "Length: 2749, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outCat = outDF.idxmax(axis=1)\n",
    "outCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For testing purposes, also extract the max value, as it can serve as an indicator as to how confident the model is sure of its predicion on the specified query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2.714901\n",
       "1       1.683838\n",
       "2       2.340028\n",
       "3       0.845122\n",
       "4       0.800888\n",
       "          ...   \n",
       "2744    2.817371\n",
       "2745    2.637920\n",
       "2746    2.986565\n",
       "2747    2.571135\n",
       "2748    2.959863\n",
       "Length: 2749, dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outValue = outDF.max(axis=1)\n",
    "outValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concat all of the data into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I started X-Slimmer at eight this morning, it`...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.714901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why won`t the kids sleep</td>\n",
       "      <td>1</td>\n",
       "      <td>1.683838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jennnnnn richhhh wast to the ed</td>\n",
       "      <td>1</td>\n",
       "      <td>2.340028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOL! I hate when that happens!! All hyped up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stuck on NJ Transit for the past twenty minute...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>Yes. Nag twitter. HAHA  Thanks. LM.</td>\n",
       "      <td>2</td>\n",
       "      <td>2.817371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>hm, this not a good medium for much more then...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.637920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>hapee mother`s day t all the mothers out there!</td>\n",
       "      <td>2</td>\n",
       "      <td>2.986565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>Aww maybe i traumatized her.</td>\n",
       "      <td>0</td>\n",
       "      <td>2.571135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>_caruso So I took the polish off of the nail o...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.959863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2749 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            DATA_COLUMN  0         1\n",
       "0     I started X-Slimmer at eight this morning, it`...  0  2.714901\n",
       "1                              why won`t the kids sleep  1  1.683838\n",
       "2                       Jennnnnn richhhh wast to the ed  1  2.340028\n",
       "3      LOL! I hate when that happens!! All hyped up ...  1  0.845122\n",
       "4     Stuck on NJ Transit for the past twenty minute...  1  0.800888\n",
       "...                                                 ... ..       ...\n",
       "2744                Yes. Nag twitter. HAHA  Thanks. LM.  2  2.817371\n",
       "2745   hm, this not a good medium for much more then...  0  2.637920\n",
       "2746    hapee mother`s day t all the mothers out there!  2  2.986565\n",
       "2747                       Aww maybe i traumatized her.  0  2.571135\n",
       "2748  _caruso So I took the polish off of the nail o...  0  2.959863\n",
       "\n",
       "[2749 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallOut = pd.concat([df['DATA_COLUMN'], outCat, outValue],axis=1)\n",
    "overallOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the dataframe into a CSV for manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallOut.to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (DEMO) graphs\n",
    "\n",
    "- a set of code to generate a graph representing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    929\n",
      "1    975\n",
      "2    845\n",
      "Name: 0, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(['negative','neutral','positive'], overallOut[0].value_counts().sort_index())\n",
    "print(overallOut[0].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Validation) Attaching the predictions to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26493</td>\n",
       "      <td>I started X-Slimmer at eight this morning, it`...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11488</td>\n",
       "      <td>why won`t the kids sleep</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7782</td>\n",
       "      <td>Jennnnnn richhhh wast to the ed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18241</td>\n",
       "      <td>LOL! I hate when that happens!! All hyped up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25540</td>\n",
       "      <td>Stuck on NJ Transit for the past twenty minute...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>11645</td>\n",
       "      <td>Yes. Nag twitter. HAHA  Thanks. LM.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>23533</td>\n",
       "      <td>hm, this not a good medium for much more then...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>6578</td>\n",
       "      <td>hapee mother`s day t all the mothers out there!</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>5338</td>\n",
       "      <td>Aww maybe i traumatized her.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>21793</td>\n",
       "      <td>_caruso So I took the polish off of the nail o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2749 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                        DATA_COLUMN  \\\n",
       "0          26493  I started X-Slimmer at eight this morning, it`...   \n",
       "1          11488                           why won`t the kids sleep   \n",
       "2           7782                    Jennnnnn richhhh wast to the ed   \n",
       "3          18241   LOL! I hate when that happens!! All hyped up ...   \n",
       "4          25540  Stuck on NJ Transit for the past twenty minute...   \n",
       "...          ...                                                ...   \n",
       "2744       11645                Yes. Nag twitter. HAHA  Thanks. LM.   \n",
       "2745       23533   hm, this not a good medium for much more then...   \n",
       "2746        6578    hapee mother`s day t all the mothers out there!   \n",
       "2747        5338                       Aww maybe i traumatized her.   \n",
       "2748       21793  _caruso So I took the polish off of the nail o...   \n",
       "\n",
       "      LABEL_COLUMN  prediction  \n",
       "0                0           0  \n",
       "1                0           1  \n",
       "2                1           1  \n",
       "3                1           1  \n",
       "4                1           1  \n",
       "...            ...         ...  \n",
       "2744             2           2  \n",
       "2745             0           0  \n",
       "2746             2           2  \n",
       "2747             0           0  \n",
       "2748             0           0  \n",
       "\n",
       "[2749 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRaw['prediction'] = outCat\n",
    "dfRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- saving it for manual/automatic validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw.to_csv('testing_set_predictions.csv')\n",
    "#dfRaw.to_csv('training_set_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
