{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet.ipynb\n",
    "\n",
    "### Designation: Model Generation Script\n",
    "\n",
    "    Purpose: to train a single BERTweet model on our dataset for training ('Tweets.csv') at 90:10 train-test split\n",
    "\n",
    "- Requirements:\n",
    "    \n",
    "    Packages: tensorflow, pandas, matplotlib, transformers, sklearn, os\n",
    "\n",
    "    Datasets (csv's): Tweets.csv\n",
    "\n",
    "- This program will require an internet connection, as it will download the model and tokenizer from the HuggingFace model repository.\n",
    "\n",
    "- Saved model-weight name (output): bertweet9010.h5\n",
    "    - Please note, all files referenced (input and output) will all be on the folder-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on BERTweet, Tensorflow and GPU:\n",
    "\n",
    "- This is a RoBERTa model, which means it makes predictions of each word based on ALL words around it in the sentence. This has the following consequences, aside from achieving high accuracy\n",
    "\n",
    "    - It has a LOT of weights to be trained. hence a GPU is strongly recommended, if I cannot 'require' it.\n",
    "\n",
    "    - It is very prone to overfitting, (although slightly better than BERT), and as such, the way to tune this model is to run 1 epoch on as many unique queries as possible.\n",
    "\n",
    "    - It will take up a lot of storage. We would even observe training speed differences if it was on a hard drive vs. if it was on an SSD.\n",
    "\n",
    "- For Tensorflow:\n",
    "    \n",
    "    - This model is compiled with Tensorflow 2.8+, with CUDA toolkit version 11.6 with its corresponding cuDNN and other nVidia required databases on python 3.8.10\n",
    "\n",
    "    - Training time reference: 12 minutes (GPU) - 1 epoch, batch size 32, 90:10 train-test ratio, RTX 3060, Ryzen 5 5600x, 8GB ram assigned, WSL-2 ubuntu 20.04 LTS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Standalone Setup\n",
    "\n",
    "- To get tensorflow working with the correct device, we import it first before we load the model.\n",
    "\n",
    "- 'useCPU' argument will disable CUDA and force TensorFlow to run with CPU only.\n",
    "    \n",
    "    - there are many 'with tf.device('/GPU:0'):' casts, forcing TensorFlow to run with CPU only here will not raise an error. \n",
    "        \n",
    "        TensorFlow will use CPU even when the code tells it to use GPU, as there is no GPU detected.\n",
    "\n",
    "            Tip: Restart the kernel before running the program (again), so no variable collision happens, and it will ensure that there are RAM/VRAM available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 18:49:25.485842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:25.742786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:25.743087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "useCPU = False #Choose whether to use CPU or GPU for running the program\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "if useCPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing, downloading, and Building the model\n",
    "\n",
    "- The model and tokenizer are quite large, so make sure enough drive space is provided in C drive, and a fast, reliable internet connection is available.\n",
    "\n",
    "- We have specified that we want, from HuggingFace, a BERTweet tokenizer (using AutoModel Casting), and the BerTweet (RoBERTa) model with a sequence classification head attached to it.\n",
    "\n",
    "    - And we specified that we are doing a 3-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-02 18:49:29.025403: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-02 18:49:29.026916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.027208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.027478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.564770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.565105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.565116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-02 18:49:29.565393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 18:49:29.565433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9579 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "2022-06-02 18:49:31.516051: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 134309376 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592899    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,902,275\n",
      "Trainable params: 134,902,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "with tf.device('/GPU:0'):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\",num_labels=3,problem_type=\"multi_label_classification\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",num_labels=3)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in dataset: Tweets.csv (our dataset for training purposes), and clean up the dataset\n",
    "\n",
    "- textID and selected_text will not matter for this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                    I`d have responded, if I were going   neutral\n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                              my boss is bullying me...  negative\n",
       "3                         what interview! leave me alone  negative\n",
       "4       Sons of ****, why couldn`t they put them on t...  negative\n",
       "...                                                  ...       ...\n",
       "27476   wish we could come see u on Denver  husband l...  negative\n",
       "27477   I`ve wondered about rake to.  The client has ...  negative\n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive\n",
       "27479                         But it was worth it  ****.  positive\n",
       "27480     All this flirting going on - The ATG smiles...   neutral\n",
       "\n",
       "[27481 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../Dataset/Tweets.csv', encoding='ISO-8859-1')\n",
    "dataset_drop = dataset.drop(['textID', 'selected_text'], axis=1)\n",
    "dataset_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Extract and encode the dataset's label column into number-category encoding.\n",
    "\n",
    "- 0 is negative, 1 is neutral, 2 is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "27476    0\n",
       "27477    0\n",
       "27478    2\n",
       "27479    2\n",
       "27480    1\n",
       "Name: sentiment, Length: 27481, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetSentimentEncode = dataset_drop['sentiment'].apply(lambda c: 0 if c == 'negative' else (1 if c=='neutral' else 2))\n",
    "datasetSentimentEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compiling Training/Test split dataframes\n",
    "\n",
    "- 90:10 seeded split, and dataframes are named in a specific way so we can use the loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                             DATA_COLUMN  LABEL_COLUMN\n",
       " 8775                                   blastinggg music.             1\n",
       " 8885    If it`s any consolation, you`re definitely on...             2\n",
       " 22325                    fun day with boo. short but fun             2\n",
       " 13024   Blow me away it IS raining harder here. Yay y...             1\n",
       " 17426   Lame remarks like 'I wonder if they like blon...             1\n",
       " ...                                                  ...           ...\n",
       " 16432                                   FC is back dear.             1\n",
       " 8964    tea...  Mmmm crispy but no cake  Have headpho...             1\n",
       " 5944                       thankyou very much, you rock!             2\n",
       " 5327                                i looking at failure             1\n",
       " 15305   happy mommas day . ging is so lucky to have a...             2\n",
       " \n",
       " [24732 rows x 2 columns],\n",
       "                                              DATA_COLUMN  LABEL_COLUMN\n",
       " 26493  I started X-Slimmer at eight this morning, it`...             0\n",
       " 11488                           why won`t the kids sleep             0\n",
       " 7782                     Jennnnnn richhhh wast to the ed             1\n",
       " 18241   LOL! I hate when that happens!! All hyped up ...             1\n",
       " 25540  Stuck on NJ Transit for the past twenty minute...             1\n",
       " ...                                                  ...           ...\n",
       " 11645                Yes. Nag twitter. HAHA  Thanks. LM.             2\n",
       " 23533   hm, this not a good medium for much more then...             0\n",
       " 6578     hapee mother`s day t all the mothers out there!             2\n",
       " 5338                        Aww maybe i traumatized her.             0\n",
       " 21793  _caruso So I took the polish off of the nail o...             0\n",
       " \n",
       " [2749 rows x 2 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(dataset_drop['text'].astype(str), datasetSentimentEncode, test_size=0.1, random_state=21)\n",
    "trainDF = pd.DataFrame()\n",
    "testDF = pd.DataFrame()\n",
    "trainDF['DATA_COLUMN'] = xtrain\n",
    "trainDF['LABEL_COLUMN'] = ytrain\n",
    "testDF['DATA_COLUMN'] = xtest\n",
    "testDF['LABEL_COLUMN'] = ytest\n",
    "trainDF,testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. (Optional) Saving the seeded testing/training data splits.\n",
    "\n",
    "- this is done for various verification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testDF.to_csv('testingData.csv')\n",
    "#trainDF.to_csv('trainingData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting dataframes into supported input format for the AI\n",
    "\n",
    "- defining methods of conversion below\n",
    "\n",
    "- The conversion will first convert the dataframe into a list of InputExample Objects\n",
    "\n",
    "    - InputExample: Hugging Face-provided object-enclosure for data.\n",
    "\n",
    "- Then it will convert the list of InputExample objects into a TensorFlow Tensor Dataset, with a list of text IDs and a list of labels.\n",
    "\n",
    "    - The tokenizer is used to convert the plain text into a list of vectors (numbers, textIDs), that the machine understands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. The call of the functions, and batching\n",
    "\n",
    "- We will transform both train and test data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(trainDF, testDF, DATA_COLUMN, LABEL_COLUMN)\n",
    "with tf.device('/GPU:0'):\n",
    "    train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "    train_data = train_data.shuffle(100).batch(32)\n",
    "\n",
    "    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "    validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model\n",
    "\n",
    "- Compiling the model, which we are using Adam, and we specify the learning rate here.\n",
    "\n",
    "- We run model.fit() (the train command), and we specify the epoch we run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 439s 554ms/step - loss: 0.5660 - accuracy: 0.7644 - val_loss: 0.4612 - val_accuracy: 0.8068\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = True #Testing: whether training the head only or the entire model would result in a better model.\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), #default: 3e-5\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "    out = model.fit(train_data, epochs=1, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Save Weights\n",
    "\n",
    "- If the training result is satisfactory, uncomment the code below and run the cell.\n",
    "    - it will grab the model weight that is still stored in the kernel and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('bertweet9010.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
