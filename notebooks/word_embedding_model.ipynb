{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# regex\n",
    "import regex as re\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dense, Dropout, SpatialDropout1D, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Flatten\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# tabulate\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing, tokenization function\n",
    "\"\"\"\n",
    "project note:\n",
    "modified version of https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script/script\n",
    "\n",
    "originally supplied by creators as GloVE as a Rudy script and modified by several contributors\n",
    "\n",
    "-----\n",
    "\n",
    "Script for preprocessing tweets by Romain Paulus\n",
    "with small modifications by Jeffrey Pennington\n",
    "with translation to Python by Motoki Wu (github.com/tokestermw)\n",
    "\n",
    "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
    "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "this version from gist.github.com/ppope > preprocess_twitter.py\n",
    "\n",
    "light edits by amackcrane\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" <hashtag> {} <allcaps> \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = ' ' + \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS)) + ' '\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \" \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    # replace apostrophes and backticks with empty string\n",
    "    text = re_sub(r\"['`]\", '')\n",
    "    # replace urls\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n",
    "    # replace @user mentions\n",
    "    text = re_sub(r\"@\\w+\", \" <user> \")\n",
    "    # replaces smiles\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n",
    "    # replaces lolfaces\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" <lolface> \")\n",
    "    # replaces sadfaces\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n",
    "    # replaces neutralfaces\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n",
    "    # replaces forward slashes with spaces around a forward slash\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    # replaces hearts\n",
    "    text = re_sub(r\"<3\",\" <heart> \")\n",
    "    # replaces numbers\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "    # replace hashtags\n",
    "    text = re_sub(r\"#\\w+\", hashtag)  \n",
    "    # replace repeated punctuation (!?.)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n",
    "    # replace repeated non whitespace (that isn't !, ?, or .)\n",
    "    text = re_sub(r\"\\b(\\S*?)(\\S)\\2{2,}\\b\", r\"\\1\\2 <elong> \")\n",
    "    # replace one character from [a-zA-Z<>()] followed by one char from [?!.:;,] with\n",
    "    #   those two characters separated by a space\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([?!.:;,])\", r\"\\1 \\2\")\n",
    "    # replace 1+ characters from [a-zA-Z<>] surrounded by () with\n",
    "    #   those characters surrounded by spaces surrounded by ()\n",
    "    text = re_sub(r\"\\(([a-zA-Z<>]+)\\)\", r\"( \\1 )\")\n",
    "    # replace 2+ uppercase letters with those letters lowercase followed by <allcaps>\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", allcaps)\n",
    "\n",
    "    # handle certain punctuation being next to letters\n",
    "    pt = r\"[@#&\\+\\$\\\\{}\\[\\]\\?!()\\^_\\-.,:';~=]\" \n",
    "    text = re_sub(rf\"([a-zA-Z])({pt}[()^<>_\\-.:';~=ox]*{pt}|{pt}+)\", r\"\\1 \\2 \")\n",
    "    text = re_sub(rf\"({pt}[()^<>_\\-.:';~=ox]*{pt}|{pt}+)([a-zA-Z])\", r\" \\1 \\2\")\n",
    "\n",
    "    # handle strings of *\n",
    "    text = re_sub(r\"(\\*+)\", r\" \\1 \")\n",
    "\n",
    "    # replace two or more spaces with one space\n",
    "    text = re_sub(r\" {2,}\", r\" \")\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../Dataset/Tweets.csv', encoding='ISO-8859-1')\n",
    "\n",
    "dataset.drop(['textID', 'selected_text'], axis=1, inplace=True)\n",
    "dataset['text'] = dataset['text'].apply(lambda t: str(t))\n",
    "\n",
    "# apply preprocessing to a dataset copy\n",
    "dataset_pre = dataset.copy(deep=True)\n",
    "\n",
    "# apply preprocessing using tokenize function\n",
    "dataset_pre['text'] = dataset_pre['text'].apply(lambda t: tokenize(t))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_pre.drop(['sentiment'], axis='columns'), dataset_pre['sentiment'], test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6852bc709a7cd20173cbeeb218505078f8f37c57",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# build vocabulary for training set\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# don't do any additional punctuation filtering, that's what the preprocessing step is for\n",
    "tokenizer.filters = '' \n",
    "\n",
    "# create a vocabulary (mapping of words to indices) for the training set\n",
    "# note that no words are removed in the vocabulary creation, there is no minimum amount of occurances needed\n",
    "tokenizer.fit_on_texts(X_train.text) \n",
    "\n",
    "# words in the vocabulary built from training set\n",
    "training_words = list(tokenizer.word_index.keys())\n",
    "\n",
    "training_vocab_size = len(training_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "45de439df3015030c71f84c2d170346936a1d68f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# transform examples into sequences of indices\n",
    "\n",
    "SEQUENCE_LENGTH = 50\n",
    "\n",
    "# texts_to_sequences turns dataframe of text into lists of vocab indices for the words in each text\n",
    "train_index_seqs = tokenizer.texts_to_sequences(X_train['text'])\n",
    "\n",
    "# pad_sequences pads the list of indices to the same length with 0's (pads on front)\n",
    "X_train_seq = pad_sequences(train_index_seqs, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Hmm..You can`t judge a book by looking at its cover\n",
      "preprocessed: hmm . <repeat> you cant judge a book by looking at its cover\n",
      "as training vocabulary indices: [802, 1, 5, 13, 59, 3330, 8, 472, 142, 255, 33, 31, 1126]\n",
      "back to preprocssed: hmm . <repeat> you cant judge a book by looking at its cover "
     ]
    }
   ],
   "source": [
    "# inspect how preprocessing and turning into a sequence works on an example\n",
    "\n",
    "train_idx = 4\n",
    "\n",
    "# original dataset entry\n",
    "dataset_idx = X_train.iloc[train_idx].name\n",
    "print('original:', dataset.text.loc[dataset_idx])\n",
    "\n",
    "# preprocessed text entry \n",
    "print('preprocessed:', X_train.text.iloc[train_idx])\n",
    "\n",
    "# preprocessed text as vocab indices\n",
    "seq = train_index_seqs[train_idx]\n",
    "print('as training vocabulary indices:', seq)\n",
    "\n",
    "# convert back from vocab indices to words of text\n",
    "print('back to preprocssed: ', end='')\n",
    "for i in seq:\n",
    "    print(tokenizer.index_word[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "04239a9bef76e7922fd86098a5601dfde8ee4665",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encode class labels numerically \n",
    "lab_encoder = LabelEncoder()\n",
    "lab_encoder.fit(y_train.unique())\n",
    "y_train_numeric = lab_encoder.transform(y_train.to_list())\n",
    "y_train_numeric = y_train_numeric.reshape(-1,1)\n",
    "\n",
    "# one hot encoding\n",
    "hot_encoder = OneHotEncoder(sparse=False)\n",
    "hot_encoder.fit(y_train_numeric)\n",
    "y_train_encoded = hot_encoder.transform(y_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "04299c886911ca135583ab64878f213939a2990c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (21984, 50)\n",
      "y_train (21984, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", X_train_seq.shape)\n",
    "print(\"y_train\", y_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the pretrained embeddings from the internet. <br>\n",
    "It downloads a zip to the directory ./pretrained_embeddings (creates it if it doesn't exist) and unzips it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVE embeddings: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "!wget -P ./pretrained_embeddings https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "!unzip -qo ./pretrained_embeddings/glove.twitter.27B.zip -d ./pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained embedding\n",
    "\n",
    "pretrained_embeddings = ['glove.twitter.27B.25d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.200d.txt']\n",
    "\n",
    "embedding_choice = 3\n",
    "\n",
    "path = './pretrained_embeddings/' + pretrained_embeddings[embedding_choice]\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=False, no_header=True)\n",
    "\n",
    "# embedding vocabulary, the words that the pretrained embedding was trained on\n",
    "embedding_words = word_vectors.index_to_key\n",
    "\n",
    "# size of each vector in embedding\n",
    "EMBEDDING_DIM = word_vectors.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape (words in embedding vocabulary, dimensions of embedding vectors): (1193514, 200)\n",
      "vector for word 'cat' (size 200): [ 0.14557  -0.47214   0.045594 -0.11133  -0.44561 ] ... [ 0.29545  -0.49186   0.24053  -0.46081  -0.077296]\n"
     ]
    }
   ],
   "source": [
    "print('embedding shape (words in embedding vocabulary, dimensions of embedding vectors):', word_vectors.vectors.shape)\n",
    "\n",
    "# vector for word 'cat'\n",
    "cat_vec_2 = word_vectors.get_vector('cat')\n",
    "print(f'vector for word \\'cat\\' (size {EMBEDDING_DIM}):', cat_vec_2[0:5], '...', cat_vec_2[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8324302434921265),\n",
       " ('cats', 0.7685185074806213),\n",
       " ('kitty', 0.750445544719696),\n",
       " ('kitten', 0.7489697933197021),\n",
       " ('pet', 0.7319862842559814),\n",
       " ('puppy', 0.7023192644119263),\n",
       " ('dogs', 0.7016381621360779),\n",
       " ('animal', 0.6421106457710266),\n",
       " ('bear', 0.6309184432029724),\n",
       " ('meow', 0.6304775476455688)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words similar to my_word art closer to it in space\n",
    "my_word = 'cat'\n",
    "word_vectors.most_similar(my_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of embedding matrix: (20710, 200)\n",
      "number of words in dataset that weren't in pretrained embedding vocabulary: 2434\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix which will make up the embedding layer of the neural network\n",
    "\n",
    "embedding_matrix = np.zeros((training_vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "# words from our dataset that aren't in the pretrained embedding's vocabulary\n",
    "not_in_embedding = []\n",
    "\n",
    "# for each word and its index in the tokenizer vocabulary (these are our words from the dataset)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\n",
    "  # if that word is in the model's mapping between words and embeddings (vectors)\n",
    "  if word in word_vectors:\n",
    "\n",
    "    # set the row of the embedding matrix corresponding to the word's index in the tokenizer vocabulary\n",
    "    #   to the embedding (vector) of that word \n",
    "    embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "  else:\n",
    "    not_in_embedding.append(word)\n",
    "    \n",
    "# for each word in the dataset, embedding_matrix has its embedding (vector) from the pretrained model\n",
    "# if the word isn't present in the pretrained embedding, the vector is all 0's\n",
    "print('dimensions of embedding matrix:', embedding_matrix.shape)\n",
    "print('number of words in dataset that weren\\'t in pretrained embedding vocabulary:', len(not_in_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding layer translates indices for words in the training vocabulary into their \n",
    "#   embedding/vector equivalents as determined by the pretrained embedding\n",
    "\n",
    "# input_dim=training_vocab_size is the number of words in our training set\n",
    "# output_dim=EMBEDDING_DIM is the dimension of the embedding, the dimension of each vector in the embedding\n",
    "# embeddings_initializer=Constant(embedding_matrix) are embeddings for each word in our training set\n",
    "# input_length=SEQUENCE_LENGTH is the size of examples once transformed, padded into lists of word indices\n",
    "# trainable=False so the embedding layer weights are not updated with training\n",
    "embedding_layer = Embedding(input_dim=training_vocab_size, output_dim=EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that creates a model with specified hyperparameters\n",
    "\n",
    "def create_model(layers_with_args,\n",
    "                 learn_rate = 1e-3,\n",
    "                 optimizer = 'Adam',\n",
    "                 momentum = 0.9,\n",
    "                 loss_fn = 'mse'):\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layers.append(embedding_layer)\n",
    "\n",
    "    for layer_type, layer_args in layers_with_args:\n",
    "\n",
    "        if layer_type == 'drp':\n",
    "            layers.append(Dropout(**layer_args))\n",
    "\n",
    "        elif layer_type == 'sdrp':\n",
    "            layers.append(SpatialDropout1D(**layer_args))\n",
    "\n",
    "        elif layer_type == 'conv':\n",
    "            layers.append(Conv1D(**layer_args))\n",
    "\n",
    "        elif layer_type == 'lstm':\n",
    "            layers.append(LSTM(**layer_args))\n",
    "\n",
    "        elif layer_type == 'blstm':\n",
    "            layers.append(Bidirectional(LSTM(**layer_args)))\n",
    "\n",
    "        elif layer_type == 'dns':\n",
    "            layers.append(Dense(**layer_args))    \n",
    "\n",
    "        elif layer_type == 'flt':\n",
    "            layers.append(Flatten(**layer_args))\n",
    "        \n",
    "        elif layer_type == 'mxp':\n",
    "            layers.append(MaxPooling1D(**layer_args))\n",
    "\n",
    "        else:\n",
    "            print('invalid layer type!')\n",
    "            return None\n",
    "\n",
    "\n",
    "    if optimizer == 'SGD':\n",
    "        opt = SGD(learning_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "    elif optimizer == 'Adam':\n",
    "        opt = Adam(learning_rate=learn_rate)\n",
    "\n",
    "    else:\n",
    "        opt = optimizer\n",
    "\n",
    "\n",
    "    model = Sequential(layers=layers)\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "\n",
    "reduce_lr_factor = 0.1\n",
    "reduce_lr_min_lr = 1e-5\n",
    "reduce_lr_patience = 5\n",
    "reduce_lr_cooldown = 0\n",
    "reduce_lr_monitor = 'val_loss'\n",
    "rlr = ReduceLROnPlateau(factor=reduce_lr_factor, min_lr=reduce_lr_min_lr, patience=reduce_lr_patience, cooldown=reduce_lr_cooldown, monitor=reduce_lr_monitor)\n",
    "\n",
    "early_stop_patience = 5\n",
    "early_stop_min_delta = 1e-4\n",
    "early_stop_monitor = 'val_accuracy'\n",
    "es = EarlyStopping(patience=early_stop_patience, min_delta=early_stop_min_delta, monitor=early_stop_monitor)\n",
    "\n",
    "callbacks = [rlr, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values for some of the hyper parameters\n",
    "\n",
    "epoch_choices = [50] #[50, 100]\n",
    "learn_rate_choices = [3e-3] #[1e-4, 3e-3, 1e-2] \n",
    "optimizer_choices = ['Adam'] #['Adamax', 'Adagrad', 'Adam', 'SGD', 'Nadam', 'Adadelta']   \n",
    "loss_choices = ['categorical_crossentropy'] #['mse', 'categorical_crossentropy'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model found by gridsearch\n",
    "# 76.1 with k fold (lr 3e-3, Adam, epochs=50, categorical_crossentropy)\n",
    "grid = [\n",
    "        {\n",
    "        'layer_order': [('drp', 'blstm', 'dns')],\n",
    "        'layer_0_args': [{'rate': 0.6}],\n",
    "        'layer_1_args': [{'units': 150}],\n",
    "        'layer_2_args': [{'units': 3, 'activation': 'sigmoid'}] \n",
    "        }\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom grid search \n",
    "\n",
    "# list of dicts\n",
    "# one dict per architecture (architecture = number and types of layers)\n",
    "# each dict specifies the different parameter values for an architecture's layers\n",
    "grid = [\n",
    "        {\n",
    "        'layer_order': [('drp', 'lstm', 'dns')],\n",
    "        'layer_0_args': [{'rate': 0.5}, \n",
    "                         {'rate': 0.2}],\n",
    "        'layer_1_args': [{'units': 100, 'dropout': 0.2, 'recurrent_dropout': 0.2},\n",
    "                         {'units': 200, 'dropout': 0.2}], \n",
    "        'layer_2_args': [{'units': 3, 'activation': 'sigmoid'}] # don't mess with the number of units in the output layer\n",
    "        },\n",
    "\n",
    "        {\n",
    "        'layer_order': [('drp', 'lstm', 'dns', 'dns')],\n",
    "        'layer_0_args': [{'rate': 0.5}, \n",
    "                         {'rate': 0.2}],\n",
    "        'layer_1_args': [{'units': 100, 'dropout': 0.2, 'recurrent_dropout': 0.2},\n",
    "                         {'units': 200, 'dropout': 0.2}], \n",
    "        'layer_2_args': [{'units': 50, 'activation': 'relu'}, {'units': 100, 'activation': 'relu'}],\n",
    "        'layer_3_args': [{'units': 3, 'activation': 'sigmoid'}] # don't mess with the number of units in the output layer\n",
    "        }\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_params = {\n",
    "                'epochs': epoch_choices,\n",
    "                'learn_rate': learn_rate_choices,\n",
    "                'optimizer': optimizer_choices,\n",
    "                'loss_fn': loss_choices\n",
    "               }\n",
    "\n",
    "# add other params dict to each dict in grid\n",
    "for d in grid:\n",
    "    d.update(other_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of models to search: 1\n",
      "\n",
      "model 1 of 1\n",
      "searching parameters: {'epochs': 50, 'layer_0_args': {'rate': 0.6}, 'layer_1_args': {'units': 150}, 'layer_2_args': {'units': 3, 'activation': 'sigmoid'}, 'layer_order': ('drp', 'blstm', 'dns'), 'learn_rate': 0.003, 'loss_fn': 'categorical_crossentropy', 'optimizer': 'Adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 00:00:58.831380: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - loss (categorical_crossentropy): 0.5875210165977478; accuracy: 76.98954343795776%\n",
      "Fold 2 - loss (categorical_crossentropy): 0.6401592493057251; accuracy: 74.57935214042664%\n",
      "Fold 3 - loss (categorical_crossentropy): 0.623387336730957; accuracy: 76.03456377983093%\n",
      "Fold 4 - loss (categorical_crossentropy): 0.5831453800201416; accuracy: 76.58026218414307%\n",
      "Fold 5 - loss (categorical_crossentropy): 0.6005998253822327; accuracy: 75.25022625923157%\n",
      "Fold 6 - loss (categorical_crossentropy): 0.5994269847869873; accuracy: 76.7060935497284%\n",
      "Fold 7 - loss (categorical_crossentropy): 0.6260656714439392; accuracy: 75.93266367912292%\n",
      "Fold 8 - loss (categorical_crossentropy): 0.6144983172416687; accuracy: 75.93266367912292%\n",
      "Fold 9 - loss (categorical_crossentropy): 0.6425366997718811; accuracy: 75.88716745376587%\n",
      "Fold 10 - loss (categorical_crossentropy): 0.617222249507904; accuracy: 75.75068473815918%\n",
      "Average accuracy: 0.760\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# do k-fold cross validation on each model searched\n",
    "# record metrics\n",
    "\n",
    "param_grid = ParameterGrid(grid)\n",
    "\n",
    "cur_model = 1\n",
    "num_models = len(param_grid)\n",
    "print('number of models to search:', num_models)\n",
    "\n",
    "histories = []\n",
    "all_scores = []\n",
    "acc_and_args = []\n",
    "\n",
    "k_fold = True\n",
    "\n",
    "# for each set of parameters in the grid search\n",
    "for p in param_grid:\n",
    "\n",
    "        print(f'\\nmodel {cur_model} of {num_models}\\nsearching parameters:', p)\n",
    "        cur_model += 1\n",
    "\n",
    "        # for each layer, combine layer type with its args \n",
    "        layer_order = p['layer_order']\n",
    "        layers_with_args = []\n",
    "        for i in range(len(layer_order)):\n",
    "\n",
    "            args_key = 'layer_' + str(i) + '_args'\n",
    "            layer_args = p[args_key]\n",
    "            layers_with_args.append((layer_order[i], layer_args))\n",
    "\n",
    "\n",
    "        # if k fold cross validation specified\n",
    "        if k_fold:\n",
    "\n",
    "            scores = {\n",
    "                'test_accuracy': [],\n",
    "                'test_error': [],\n",
    "                'test_precision_neg': [],\n",
    "                'test_precision_neut': [],\n",
    "                'test_precision_pos': [],\n",
    "                'test_recall_neg': [],\n",
    "                'test_recall_neut': [],\n",
    "                'test_recall_pos': [],\n",
    "                'test_f1_score_neg': [],\n",
    "                'test_f1_score_neut': [],\n",
    "                'test_f1_score_pos': []\n",
    "                    }\n",
    "\n",
    "            fold_num = 1\n",
    "\n",
    "            kfold = KFold(n_splits=10)\n",
    "\n",
    "            # for each fold, train a model on the other folds and evaluate on the test fold\n",
    "            for train, test in kfold.split(X_train_seq, y_train_encoded):\n",
    "\n",
    "                # configure\n",
    "                model = create_model(layers_with_args=layers_with_args, learn_rate=p['learn_rate'], optimizer=p['optimizer'], loss_fn=p['loss_fn'])\n",
    "                \n",
    "                # train\n",
    "                history = model.fit(X_train_seq[train], y_train_encoded[train], batch_size=128, epochs=p['epochs'], \n",
    "                                    validation_data=(X_train_seq[test], y_train_encoded[test]), callbacks=callbacks, verbose=0)\n",
    "\n",
    "                # evaluate\n",
    "                error, acc = model.evaluate(X_train_seq[test], y_train_encoded[test], verbose=0)\n",
    "\n",
    "                # save scores for later classification report\n",
    "                preds = np.argmax(model.predict(X_train_seq[test]), axis=1).reshape(-1, 1)\n",
    "                y_true = hot_encoder.inverse_transform(y_train_encoded[test])\n",
    "\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(y_true, preds)\n",
    "\n",
    "                scores['test_accuracy'].append(acc)\n",
    "                scores['test_error'].append(error)\n",
    "                scores['test_precision_neg'].append(precision[0])\n",
    "                scores['test_precision_neut'].append(precision[1])\n",
    "                scores['test_precision_pos'].append(precision[2])\n",
    "                scores['test_recall_neg'].append(recall[0])\n",
    "                scores['test_recall_neut'].append(recall[1])\n",
    "                scores['test_recall_pos'].append(recall[2])\n",
    "                scores['test_f1_score_neg'].append(f1[0])\n",
    "                scores['test_f1_score_neut'].append(f1[1])\n",
    "                scores['test_f1_score_pos'].append(f1[2])\n",
    "\n",
    "                # print error and accuracy for this fold\n",
    "                print(f'Fold {fold_num} - {model.metrics_names[0]} ({p[\"loss_fn\"]}): {error}; {model.metrics_names[1]}: {acc*100}%')\n",
    "\n",
    "                fold_num = fold_num + 1\n",
    "\n",
    "\n",
    "            avg_acc = np.mean(scores['test_accuracy'])\n",
    "            print('Average accuracy: %.3f' % avg_acc)\n",
    "            histories.append((avg_acc, p))\n",
    "            all_scores.append((scores, p))\n",
    "\n",
    "        # no k fold cross validation\n",
    "        else:\n",
    "            model = create_model(layers_with_args=layers_with_args, learn_rate=p['learn_rate'], optimizer=p['optimizer'], loss_fn=p['loss_fn'])\n",
    "\n",
    "            # train model\n",
    "            history = model.fit(X_train_seq, y_train_encoded, batch_size=128, epochs=p['epochs'],\n",
    "                                validation_split=0.2, callbacks=callbacks, verbose=0)\n",
    "\n",
    "            # print the average validation accuracy of the last 6 epochs (because early stopping after 5 epochs of no improvement)\n",
    "            # if it is more than threshold, save the model arguments\n",
    "            threshold = 0.745\n",
    "            avg_acc = np.mean(history.history['val_accuracy'][-7:-1])\n",
    "            print('avg validation accuracy of last 6 epochs:', avg_acc)\n",
    "            if avg_acc > threshold:\n",
    "                acc_and_args.append((avg_acc, p))\n",
    "            \n",
    "            # save metrics\n",
    "            histories.append((avg_acc, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print out a classification report\n",
    "# can't use normal classification_report since we want to display our values from cross validation\n",
    "def custom_classif_report(metric_scores, just_acc=False):\n",
    "\n",
    "  scores_df = pd.DataFrame(metric_scores)\n",
    "  \n",
    "  # average of each metric over the splits of cross validation\n",
    "  scores_mean = scores_df.mean() \n",
    "  \n",
    "  # overall averages for precision, recall, and f1 across class labels\n",
    "  avg_precision = scores_mean[['test_precision_neut', 'test_precision_pos', 'test_precision_neg']].mean()\n",
    "  avg_recall = scores_mean[['test_recall_neut', 'test_recall_pos', 'test_recall_neg']].mean()\n",
    "  avg_f1 = scores_mean[['test_f1_score_neut', 'test_f1_score_pos', 'test_f1_score_neg']].mean()\n",
    "  \n",
    "  print('Classification Report:')\n",
    "\n",
    "  # precision, recall, f1 metrics in table printable form\n",
    "  metric_info = {\n",
    "    'precision': [scores_mean['test_precision_neut'], scores_mean['test_precision_pos'], scores_mean['test_precision_neg'], avg_precision], \n",
    "    'recall': [scores_mean['test_recall_neut'], scores_mean['test_recall_pos'], scores_mean['test_recall_neg'], avg_recall], \n",
    "    'f1-score': [scores_mean['test_f1_score_neut'], scores_mean['test_f1_score_pos'], scores_mean['test_f1_score_neg'], avg_f1]}\n",
    "  \n",
    "  # print table for precision, recall, f1\n",
    "  if not just_acc:\n",
    "    print(tabulate(metric_info, headers='keys', tablefmt='fancy_grid', showindex=['neutral', 'positive', 'negative', 'average']))\n",
    "  \n",
    "  # print table for accuracy\n",
    "  acc_info = {'accuracy': [scores_mean['test_accuracy']]}\n",
    "  print(tabulate(acc_info, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "╒══════════╤═════════════╤══════════╤════════════╕\n",
      "│          │   precision │   recall │   f1-score │\n",
      "╞══════════╪═════════════╪══════════╪════════════╡\n",
      "│ neutral  │    0.726407 │ 0.739474 │   0.732692 │\n",
      "├──────────┼─────────────┼──────────┼────────────┤\n",
      "│ positive │    0.813657 │ 0.797405 │   0.80531  │\n",
      "├──────────┼─────────────┼──────────┼────────────┤\n",
      "│ negative │    0.750257 │ 0.747346 │   0.7483   │\n",
      "├──────────┼─────────────┼──────────┼────────────┤\n",
      "│ average  │    0.76344  │ 0.761408 │   0.762101 │\n",
      "╘══════════╧═════════════╧══════════╧════════════╛\n",
      "╒════════════╕\n",
      "│   accuracy │\n",
      "╞════════════╡\n",
      "│   0.759643 │\n",
      "╘════════════╛\n"
     ]
    }
   ],
   "source": [
    "# classification report for first model of grid search\n",
    "custom_classif_report(all_scores[0][0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3a5ec43c2134eab692cc945203422f050c3bda7e5881ff7e11f810cfd5e42c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ECS171_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
